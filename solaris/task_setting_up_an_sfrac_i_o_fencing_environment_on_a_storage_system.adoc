---
permalink: solaris/task_setting_up_an_sfrac_i_o_fencing_environment_on_a_storage_system.html 
sidebar: sidebar 
keywords: snap, snapdrive, sfrac, fencing, environment, storage 
summary: SnapDrive per UNIX offre opzioni di provisioning dello storage e gestione Snapshot per gestire i gruppi di dischi locali e condivisi a livello di cluster host e i file system in un ambiente SFRAC. 
---
= Impostazione di un ambiente di i/o scherma SFRAC su un sistema storage
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
SnapDrive per UNIX offre opzioni di provisioning dello storage e gestione Snapshot per gestire i gruppi di dischi locali e condivisi a livello di cluster host e i file system in un ambiente SFRAC.

.Fasi
. Configurazione `rsh` oppure `ssh` manualmente per utilizzare il prompt di accesso senza password per l'utente root su tutti i nodi del cluster host.
+
Per istruzioni sull'installazione, consultare la _Veritas Cluster Server Installation Guide for Solaris_.

. Installare SnapDrive per UNIX su tutti i nodi del cluster host.
+
Se su nodi diversi sono installate versioni diverse di SnapDrive per UNIX, le operazioni di SnapDrive per UNIX non riescono.

. Controllare la connettività FC tra i sistemi storage.
+
Per ulteriori informazioni sui requisiti hardware per gli host, consulta le _Note di release_ di SFRAC.

+

NOTE: Il `/opt/NTAPsnapdrive/snapdrive.conf` file su tutti i nodi deve avere `default-transport` Variabile di configurazione impostata su FCP.

. Impostare un valore per `_secure-communication-among-cluster-nodes_` variabile di configurazione, per garantire che il `rsh` oppure `ssh` access-without-password-prompt per l'utente root è configurato per tutti i nodi nel cluster.
+
Questo valore è necessario perché, se si avviano i comandi SnapDrive per UNIX da qualsiasi nodo (master o non master) nel cluster host, SnapDrive per UNIX esegue operazioni su altri nodi nel cluster host.

. Verificare la presenza del rilevamento dei dispositivi sui nodi del cluster host eseguendo il seguente comando su ciascun nodo del cluster host:
+
`*snapdrive storage create -lun _long_lun_name_ [_lun_name..._] -lunsize size [{_-reserve_ | _-noreserve_}] [_-igroup ig_name_ [_ig_name ..._]]*`

+
[listing]
----

	# snapdrive storage create -lun f270-197-109:/vol/vol2/luntest -lunsize 20m

		LUN f270-197-109:/vol/vol2/luntest ... created

		mapping new lun(s) ... done
		discovering new lun(s) ... done

		LUN to device file mappings:
	-f270-197-109:/vol/vol2/luntest => /dev/vx/dmp/c5t0d6s2

	# snapdrive storage delete -lun f270-197-109:/vol/vol2/luntest -lunsize 20m
	- LUN f270-197-109:/vol/vol2/luntest ... deleted
----

